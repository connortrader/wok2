#!/usr/bin/env python3
"""arXiv Morning Digest ‚Äî reads full paper text via arXiv HTML, powered by Gemini."""

import json
import os
import re
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone

import requests
from google import genai
from google.genai import types

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
QUANT_CATEGORIES  = ["q-fin.CP", "q-fin.PM", "q-fin.ST", "q-fin.RM", "q-fin.TR"]
AI_CATEGORIES     = ["cs.AI", "cs.LG"]
HOURS_BACK        = 96
MAX_QUANT         = 40
MAX_AI            = 15
MAX_LONGEVITY     = 10

# Strong keywords ‚Äî one match in title OR abstract is enough
LONGEVITY_STRONG = [
    "longevity", "lifespan", "healthspan", "life span", "life extension",
    "senolytic", "senolytics", "senomorphic", "cellular senescence",
    "rejuvenation", "epigenetic reprogramming", "partial reprogramming",
    "epigenetic clock", "biological age", "methylation age", "dunedinpace",
    "NAD+", "NMN", "nicotinamide mononucleotide", "NR supplementation",
    "sirtuin", "SIRT1", "SIRT3", "SIRT6",
    "rapamycin", "mTOR inhibition", "mTORC1",
    "autophagy induction", "mitophagy",
    "telomere length", "telomerase activation",
    "caloric restriction", "calorie restriction", "intermittent fasting longevity",
    "metformin aging", "acarbose", "spermidine supplementation",
    "fisetin", "quercetin senolytic", "dasatinib",
    "geroprotector", "geroprotective",
    "stem cell rejuvenation", "tissue rejuvenation",
    "hallmarks of aging", "aging hallmarks",
    "inflammaging", "chronic low-grade inflammation aging",
    "mitochondrial dysfunction aging",
    "proteostasis aging", "protein aggregation aging",
]

# Weak keywords ‚Äî need 2+ matches, or 1 match in title
LONGEVITY_WEAK = [
    "aging", "ageing", "senescence", "age-related", "age related",
    "lifespan extension", "healthspan extension",
    "telomere", "autophagy", "mTOR",
    "NAD", "sirtuins", "mitochondria", "oxidative stress",
    "gut microbiome", "sleep quality", "exercise aging",
    "IGF-1", "growth hormone", "insulin signaling",
    "DNA damage repair", "genomic instability",
]


def longevity_score(title: str, abstract: str) -> int:
    """Return relevance score for longevity filtering. 0 = not relevant."""
    title_l    = title.lower()
    abstract_l = abstract.lower()
    score = 0
    # Strong keyword in title = very high signal
    for kw in LONGEVITY_STRONG:
        if kw.lower() in title_l:
            score += 3
        elif kw.lower() in abstract_l:
            score += 1
    # Weak keyword only if also supported by strong signal
    for kw in LONGEVITY_WEAK:
        if kw.lower() in title_l:
            score += 1
    return score
GEMINI_MODEL      = "gemini-2.5-flash-lite"
OUTPUT_FILE       = "docs/index.html"
FULL_TEXT_TIMEOUT = 10    # seconds per paper HTML fetch
FULL_TEXT_CHARS   = 3000  # chars extracted per paper

TRADER_PROFILE = """
Sa kirjutad EESTI KEELES hommikuse kokkuv√µtte kvantitatiivse kaupleja jaoks.
Sul on iga artikli T√ÑISTEKST (sissejuhatus + tulemused + j√§reldused), mitte ainult abstrakt.
Kasuta TEGELIKKE NUMBREID ja T√ÑPSEID MEETODEID tekstist ‚Äî mitte √ºldisi kokkuv√µtteid.

KAUPLEJA PROFIIL:
- Kaupleb ainult USA aktsiatega. Ei tegele kr√ºpto, forex, optsioonid, futuurid, HFT, v√§listurud.
- Kasutab RealTest (Marsten Parker) + Norgate p√§eval√µpu andmed (hind, maht, fundamentaalid)
- Eesm√§rgid: mitmekesine strateegiate portfell, robustne backtesting, signaalide genereerimine
- Python: kesktase. Suudab kodeerida m√µ√µdukalt keerulisi strateegiaid.
- Huvid: momentum, mean reversion, faktormudelid, portfelli optimeerimine, drawdown kontroll

KIRJUTAMISREEGLID ‚Äî ole √úLIT√ÑPNE, kasuta numbreid tekstist:

  avastus: 2-3 lauset. Mis t√§pselt tehti ja mis tulemus saadi.
    HEA N√ÑIDE: "Testiti momentum strateegiat S&P 500 aktsiadel aastatel 1990-2020.
    Entry: aktsia sulgemine 52-n√§dala k√µrgeima taseme 5% piires. Hold: 6 kuud.
    Tulemus: Sharpe 0.87 vs buy-and-hold 0.43. Max drawdown -23% vs -51%."
    HALB N√ÑIDE: "Leiti et momentum t√∂√∂tab h√§sti." ‚Äî KEELATUD.

  selgitus: 1-2 lauset. Mida see konkreetselt selle kaupleja jaoks t√§hendab.

  toiming: √úks t√§pne tegevus. Kui artiklis on backtest, anna T√ÑPSED reeglid:
    ‚Ä¢ "Testi RealTest-is: Entry kui [t√§pne tingimus], Filter [t√§pne filter],
      Stop loss [t√§pne reegel], Hold [aeg]. Tulemus paberis: Sharpe X, CAGR Y%."
    ‚Ä¢ "Proovi t√∂√∂riista: [nimi] ‚Äî [mida teeb] ‚Äî [link kui olemas]"
    ‚Ä¢ "√ïpi: [konkreetne tehnika] ‚Äî [miks kasulik, mis kasu]"
    ‚Ä¢ "J√§ta vahele ‚Äî [√ºks konkreetne p√µhjus]"

SKOOR (1-10, summa):
  Implementeeritavus (0-3): kas saab testida RealTest-is Norgate hind/maht/fundamentaalidega?
  Leiuse kvaliteet   (0-3): kas on konkreetsed m√µ√µdetavad tulemused?
  Robustsus          (0-2): kas testiti mitut perioodi v√µi out-of-sample?
  Uudsus             (0-2): kas on uus idee selle kaupleja jaoks?

Tagasta AINULT kehtiv JSON ‚Äî ilma markdown-ita, ilma lisatekstita.
{"papers":[{"id":"...","title":"...","url":"...","category":"quant or ai","score":7,
"avastus":"...","selgitus":"...","toiming":"...","can_implement":true,"tags":["momentum"]}]}

Sorteeri skoori j√§rgi kahanevalt.
Anal√º√ºsi AINULT minu antud artikleid. √ÑRA lisa artikleid oma teadmistest.
""".strip()


LONGEVITY_PROFILE = """
Sa kirjutad EESTI KEELES hommikuse longevity/tervise kokkuv√µtte inimesele kelle eesm√§rk
on elada v√µimalikult kaua ja tervena (Bryan Johnson stiil ‚Äî superhuman biohacking).
Sul on iga artikli T√ÑISTEKST. Kasuta TEGELIKKE NUMBREID ja T√ÑPSEID LEIDE.

LUGEJA PROFIIL:
- Eesm√§rk: maksimaalne eluiga, optimaalne tervis, bioloogilise vanuse v√§hendamine
- Huvid: senolytics, NAD+/NMN/NR, rapamycin, epigeneetilised kellad, autophagy,
  p√µletik, mitokondrid, soolestiku mikrobioom, uni, treening, toitumine
- K√ºsib alati: "Kas see t√∂√∂tab inimestel? Mida ma saan T√ÑNA teha?"

KIRJUTAMISREEGLID:
  avastus: 2-3 lauset. T√ÑPSED leiud numbritega.
    - Mis organism (hiired? inimesed? rakud?), mis vanusevahemik
    - Mis interventsioon (annus, kestus, meetod)
    - Mis tulemus (% eluea pikendus, biomarkeri muutus, haiguse risk jne)
    HEA: "Hiirtel (18 kuu vanused) v√§hendas rapamycin 4mg/kg/p√§evas 12 n√§dalat
    p√µletikun√§itajaid (IL-6) 43% ja pikendas √ºlej√§√§nud eluiga 9% (p<0.01)."
    HALB: "Leiti et rapamycin v√µib pikendada eluiga." ‚Äî KEELATUD.

  selgitus: 1-2 lauset. Kas see on inimestele rakendatav? Mis piirangud?

  toiming: √úks konkreetne tegevus:
    ‚Ä¢ "Testi enda peal: [t√§pne protokoll ‚Äî annus, aeg, mida m√µ√µta]"
    ‚Ä¢ "J√§lgi uuringut: [mida oodata, millal tulemused]"
    ‚Ä¢ "Proovi t√∂√∂riista/testi: [mis test/t√∂√∂riist + link]"
    ‚Ä¢ "√ïpi: [konkreetne mehhanism miks see oluline]"
    ‚Ä¢ "J√§ta vahele ‚Äî [p√µhjus miks ei rakendu]"

SKOOR (1-10):
  Inimestele rakendatavus (0-3): kas t√∂√∂tab inimestel (mitte ainult hiirtel/rakkudel)?
  Leiu kvaliteet       (0-3): konkreetsed numbrid ja mehhanismid?
  Robustsus            (0-2): RCT v√µi suur kohort? Replikeeritud?
  Uudsus               (0-2): uus tipu/protokoll/mehhanism?

Tagasta AINULT kehtiv JSON, ilma markdown-ita.
{"papers":[{"id":"...","title":"...","url":"...","category":"longevity","score":8,
"avastus":"...","selgitus":"...","toiming":"...","can_implement":true,"tags":["NAD+"]}]}

Sorteeri skoori j√§rgi kahanevalt.
ANAL√ú√úSI AINULT minu antud artikleid.
""".strip()


# ‚îÄ‚îÄ arXiv fetch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_arxiv(categories: list, max_results: int = 80) -> list:
    cat_query = "+OR+".join(f"cat:{c}" for c in categories)
    url = (f"https://export.arxiv.org/api/query"
           f"?search_query={cat_query}"
           f"&sortBy=submittedDate&sortOrder=descending"
           f"&max_results={max_results}")
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    ns = {"a": "http://www.w3.org/2005/Atom"}
    root = ET.fromstring(resp.content)
    papers = []
    for entry in root.findall("a:entry", ns):
        raw_id = (entry.find("a:id", ns).text or "").strip()
        papers.append({
            "id":        raw_id.split("/abs/")[-1],
            "url":       raw_id,
            "title":     (entry.find("a:title", ns).text or "").strip().replace("\n", " "),
            "abstract":  (entry.find("a:summary", ns).text or "").strip().replace("\n", " "),
            "published": (entry.find("a:published", ns).text or ""),
        })
    return papers


def filter_recent(papers: list, hours: int) -> list:
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
    result = []
    for p in papers:
        try:
            pub = datetime.fromisoformat(p["published"].replace("Z", "+00:00"))
            if pub >= cutoff:
                result.append(p)
        except Exception:
            pass
    return result


# ‚îÄ‚îÄ bioRxiv / medRxiv fetch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _fetch_rxiv(server: str, start_date: str, end_date: str, scan_limit: int = 400) -> list:
    """Fetch and score papers from biorxiv or medrxiv by longevity relevance."""
    scored, cursor = [], 0
    while cursor < scan_limit:
        url = f"https://api.biorxiv.org/details/{server}/{start_date}/{end_date}/{cursor}/json"
        try:
            resp = requests.get(url, timeout=30)
            resp.raise_for_status()
            data = resp.json()
        except Exception:
            break
        collection = data.get("collection", [])
        if not collection:
            break
        for item in collection:
            title    = item.get("title", "").strip()
            abstract = item.get("abstract", "").strip()
            sc = longevity_score(title, abstract)
            if sc >= 2:          # minimum relevance threshold
                doi = item.get("doi", "")
                scored.append({
                    "_score":    sc,
                    "id":        doi,
                    "url":       f"https://www.{server}.org/content/{doi}",
                    "title":     title,
                    "abstract":  abstract.replace("\n", " "),
                    "published": item.get("date", ""),
                    "content":   abstract[:2000],
                    "is_full":   False,
                    "source":    server,
                })
        total = int((data.get("messages") or [{}])[0].get("total", 0))
        cursor += 100
        if cursor >= total:
            break
    # Sort by relevance score, return all (caller will merge + limit)
    scored.sort(key=lambda x: -x["_score"])
    return scored


def fetch_longevity_papers(days_back: int = 4, max_results: int = 12) -> list:
    """Fetch and rank longevity papers from bioRxiv + medRxiv by relevance."""
    end_date   = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    start_date = (datetime.now(timezone.utc) - timedelta(days=days_back)).strftime("%Y-%m-%d")

    bio = _fetch_rxiv("biorxiv", start_date, end_date, scan_limit=500)
    time.sleep(1)
    med = _fetch_rxiv("medrxiv", start_date, end_date, scan_limit=500)

    # Merge, deduplicate, re-sort by score (medRxiv gets +1 bonus ‚Äî human data)
    seen, merged = set(), []
    for p in med:
        if p["id"] not in seen:
            seen.add(p["id"])
            p["_score"] += 1   # human clinical data bonus
            merged.append(p)
    for p in bio:
        if p["id"] not in seen:
            seen.add(p["id"])
            merged.append(p)

    merged.sort(key=lambda x: -x["_score"])
    top = merged[:max_results]
    # Remove internal score key before returning
    for p in top:
        p.pop("_score", None)
    return top


# ‚îÄ‚îÄ Full text extraction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_text(html: str) -> str:
    """Strip HTML and extract intro + results/conclusion from arXiv paper."""
    # Remove noise
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>',  '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<math[^>]*>.*?</math>',    '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<figure[^>]*>.*?</figure>', '', html, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'<[^>]+>', ' ', html)
    text = re.sub(r'\s+', ' ', text).strip()

    if len(text) < 300:
        return text

    t_lower = text.lower()

    # Find the best conclusion/results section (must be in second half)
    midpoint = len(text) // 2
    markers = [
        'conclusion', 'conclusions', 'in summary', 'we conclude',
        'experimental results', 'our results', 'findings', 'we show that',
        'we find that', 'we demonstrate', 'results show',
    ]
    best_idx = -1
    for m in markers:
        idx = t_lower.find(m, midpoint)
        if idx > 0 and (best_idx == -1 or idx < best_idx):
            best_idx = idx

    intro       = text[:1800]
    conclusion  = text[best_idx: best_idx + 3000] if best_idx > 0 else text[-3000:]
    combined    = intro + "\n\n[...]\n\n" + conclusion
    return combined[:FULL_TEXT_CHARS]


def fetch_full_text(paper: dict) -> tuple:
    """Try to get full paper text from arXiv HTML. Returns (content, is_full_text)."""
    base_id = re.sub(r'v\d+$', '', paper['id'])
    url = f"https://arxiv.org/html/{base_id}"
    try:
        resp = requests.get(url, timeout=FULL_TEXT_TIMEOUT,
                            headers={"User-Agent": "arxiv-digest-bot/1.0 (research tool)"})
        if resp.status_code == 200 and len(resp.text) > 3000:
            extracted = extract_text(resp.text)
            if len(extracted) > 400:
                return (extracted, True)
    except Exception:
        pass
    return (paper['abstract'][:800], False)


# ‚îÄ‚îÄ Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_prompt(papers: list, profile: str = None) -> str:
    lines = [profile or TRADER_PROFILE, ""]
    for p in papers:
        source_label = "[FULL TEXT]" if p.get("is_full") else "[ABSTRACT ONLY]"
        lines += [
            "=" * 55,
            f"{source_label} {p.get('category','').upper()} | ID: {p['id']}",
            f"Title: {p['title']}",
            f"URL: {p['url']}",
            f"Content:\n{p['content']}",
            "",
        ]
    return "\n".join(lines)


def extract_json(text: str) -> dict:
    """Robustly extract JSON from Gemini response, handling extra text."""
    text = text.strip()
    # Strategy 1: direct parse
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    # Strategy 2: find { and match closing brace by counting
    start = text.find("{")
    if start >= 0:
        depth = 0
        for i, ch in enumerate(text[start:], start):
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    try:
                        return json.loads(text[start: i + 1])
                    except json.JSONDecodeError:
                        break
    raise ValueError(f"No valid JSON found in response. First 300 chars: {text[:300]}")


def normalize_result(raw) -> dict:
    """Ensure result is always {"papers": [list of dicts]}."""
    if isinstance(raw, list):
        result = {"papers": raw}
    elif isinstance(raw, dict):
        # Gemini sometimes returns {"paper": [...]} or {"results": [...]}
        if "papers" not in raw:
            for key in raw:
                if isinstance(raw[key], list):
                    result = {"papers": raw[key]}
                    break
            else:
                result = raw
        else:
            result = raw
    else:
        result = {"papers": []}
    # Filter out any non-dict items Gemini may have mixed in (e.g. "...remaining omitted")
    result["papers"] = [p for p in result.get("papers", []) if isinstance(p, dict)]
    return result


def call_gemini(prompt: str) -> dict:
    api_key = os.environ.get("GEMINI_API_KEY", "")
    if not api_key:
        raise EnvironmentError("GEMINI_API_KEY is not set.")
    client = genai.Client(api_key=api_key)
    resp = client.models.generate_content(
        model=GEMINI_MODEL,
        contents=prompt,
        config=types.GenerateContentConfig(
            response_mime_type="application/json",
            temperature=0.1,
            max_output_tokens=16000,
        ),
    )
    if not resp.text:
        finish = (resp.candidates[0].finish_reason if resp.candidates else "unknown")
        raise ValueError(f"Gemini returned empty response. finish_reason={finish}")
    return normalize_result(extract_json(resp.text))


# ‚îÄ‚îÄ HTML rendering ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def score_style(score: int) -> tuple:
    if score >= 8: return ("#e8f5e9", "#2e7d32")
    if score >= 6: return ("#fff8e1", "#e65100")
    if score >= 4: return ("#f5f5f5", "#757575")
    return ("#fafafa", "#bdbdbd")


def render_card(p: dict) -> str:
    score = p.get("score", 0)
    bg, fg = score_style(score)
    cat = p.get("category", "")
    cat_label = "QUANT" if cat == "quant" else "AI"
    cat_color = "#1565c0" if cat == "quant" else "#6a1b9a"
    can = p.get("can_implement", False)
    impl_color = "#2e7d32" if can else "#9e9e9e"
    impl_text  = "Implementeeritav RealTest-is" if can else "Ei ole otseselt implementeeritav"
    tags = " ".join(
        f'<span style="background:#f0f0f0;color:#777;padding:1px 7px;border-radius:3px;font-size:11px;">{t}</span>'
        for t in p.get("tags", [])
    )
    return f"""<div style="background:#fff;border:1px solid #e8e8e8;border-left:3px solid {fg};border-radius:6px;padding:20px 22px;margin-bottom:12px;">
  <div style="display:flex;align-items:center;gap:8px;margin-bottom:14px;flex-wrap:wrap;">
    <span style="background:{bg};color:{fg};font-weight:700;padding:2px 10px;border-radius:4px;font-size:13px;">{score}/10</span>
    <span style="color:{cat_color};font-size:11px;font-weight:700;letter-spacing:1px;border:1px solid {cat_color};padding:1px 7px;border-radius:3px;">{cat_label}</span>
    <a href="{p.get('url','#')}" target="_blank" rel="noopener"
       style="color:#111;font-weight:600;font-size:15px;text-decoration:none;line-height:1.4;">{p.get('title','')}</a>
  </div>
  <div style="border-top:1px solid #f2f2f2;padding-top:12px;">
    <div style="display:grid;grid-template-columns:100px 1fr;gap:0;margin-bottom:2px;">
      <span style="font-size:10px;font-weight:700;text-transform:uppercase;letter-spacing:1px;color:#bbb;padding:10px 0;">Avastus</span>
      <span style="font-size:14px;color:#111;line-height:1.7;padding:10px 0;border-bottom:1px solid #f5f5f5;">{p.get('avastus','')}</span>
    </div>
    <div style="display:grid;grid-template-columns:100px 1fr;gap:0;margin-bottom:2px;">
      <span style="font-size:10px;font-weight:700;text-transform:uppercase;letter-spacing:1px;color:#bbb;padding:10px 0;">T√§hendus</span>
      <span style="font-size:13px;color:#444;line-height:1.65;padding:10px 0;border-bottom:1px solid #f5f5f5;">{p.get('selgitus','')}</span>
    </div>
    <div style="display:grid;grid-template-columns:100px 1fr;gap:0;">
      <span style="font-size:10px;font-weight:700;text-transform:uppercase;letter-spacing:1px;color:#bbb;padding:10px 0;">Toiming</span>
      <span style="font-size:13px;color:#1b5e20;line-height:1.7;padding:10px 0;font-weight:500;">{p.get('toiming','')}</span>
    </div>
  </div>
  <div style="margin-top:10px;display:flex;align-items:center;gap:8px;flex-wrap:wrap;">
    <span style="font-size:11px;color:{impl_color};">‚óè {impl_text}</span>
    <span style="color:#e0e0e0;">|</span>
    {tags}
  </div>
</div>"""


def render_row(p: dict) -> str:
    score = p.get("score", 0)
    bg, fg = score_style(score)
    title = p.get("title", "")[:105]
    toiming = p.get("toiming", "")[:150]
    return f"""<div style="display:flex;gap:12px;padding:10px 4px;border-bottom:1px solid #f5f5f5;align-items:flex-start;">
  <span style="background:{bg};color:{fg};font-weight:700;padding:1px 8px;border-radius:3px;font-size:11px;white-space:nowrap;flex-shrink:0;">{score}/10</span>
  <div>
    <a href="{p.get('url','#')}" target="_blank" rel="noopener"
       style="color:#444;font-size:13px;text-decoration:none;font-weight:500;">{title}{'‚Ä¶' if len(p.get('title',''))>105 else ''}</a>
    <div style="font-size:11px;color:#999;margin-top:3px;">{toiming}{'‚Ä¶' if len(p.get('toiming',''))>150 else ''}</div>
  </div>
</div>"""


def render_ai_link(p: dict) -> str:
    return (f'<div style="padding:7px 0;border-bottom:1px solid #f5f5f5;">'
            f'<a href="{p.get("url","#")}" target="_blank" rel="noopener" '
            f'style="color:#444;font-size:13px;text-decoration:none;">{p.get("title","")}</a>'
            f'</div>')


def generate_html(quant_result: dict, ai_result: dict, bio_result: dict,
                  quant_count: int, ai_count: int, bio_count: int,
                  full_text_count: int, total_papers: int) -> str:
    all_papers = (quant_result.get("papers", []) +
                  ai_result.get("papers", []) +
                  bio_result.get("papers", []))
    top   = [p for p in all_papers if p.get("score", 0) >= 7]
    quant = [p for p in quant_result.get("papers", []) if p.get("score", 0) < 7]
    ai    = [p for p in ai_result.get("papers", []) if p.get("score", 0) < 7]
    bio   = [p for p in bio_result.get("papers", []) if p.get("score", 0) < 7]

    def section(items):
        if not items:
            return '<p style="color:#ccc;font-size:13px;padding:12px 0;">T√§na artikleid pole.</p>'
        return "\n".join(render_card(p) if p.get("score", 0) >= 5 else render_row(p) for p in items)

    date_str = datetime.now(timezone.utc).strftime("%B %d, %Y")
    weekday  = datetime.now(timezone.utc).strftime("%A")
    time_str = datetime.now(timezone.utc).strftime("%H:%M UTC")

    bio_section = f"""
<h2>Longevity &amp; Tervis ‚Äî bioRxiv</h2>
{section(bio)}
""" if bio_count > 0 else ""

    return f"""<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Morning Digest ¬∑ {date_str}</title>
<style>
  *{{margin:0;padding:0;box-sizing:border-box}}
  body{{background:#f6f6f4;color:#111;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;padding:40px 24px;max-width:760px;margin:0 auto;line-height:1.5}}
  h2{{font-size:10px;font-weight:700;text-transform:uppercase;letter-spacing:2.5px;color:#aaa;margin:44px 0 16px;padding-bottom:10px;border-bottom:1px solid #e4e4e4}}
  @media(max-width:600px){{body{{padding:20px 14px}}}}
</style>
</head>
<body>

<div style="border-bottom:2px solid #111;padding-bottom:20px;margin-bottom:4px;">
  <div style="font-size:10px;color:#aaa;text-transform:uppercase;letter-spacing:2px;margin-bottom:6px;">{weekday} ¬∑ arXiv Morning Digest</div>
  <div style="font-size:28px;font-weight:700;letter-spacing:-0.5px;margin-bottom:14px;">{date_str}</div>
  <div style="display:flex;gap:16px;flex-wrap:wrap;font-size:13px;color:#666;">
    <span>üìä {quant_count} quant</span>
    <span>ü§ñ {ai_count} AI</span>
    <span>üß¨ {bio_count} longevity</span>
    <span>üìÑ {full_text_count}/{total_papers} full text</span>
    <span>‚≠ê {len(top)} top picks</span>
    <span style="color:#bbb;">Generated {time_str}</span>
  </div>
</div>

<h2>Top Picks ‚Äî Worth your time</h2>
{section(top) if top else '<p style="color:#ccc;font-size:13px;padding:12px 0;">T√§na k√µrgeid skoore pole.</p>'}

<h2>Kvantitatiivne rahandus ‚Äî Remaining</h2>
{section(quant)}

<h2>AI &amp; Automatiseerimine ‚Äî Remaining</h2>
{section(ai)}
{bio_section}
<div style="margin-top:56px;padding-top:16px;border-top:1px solid #e8e8e8;font-size:10px;color:#ccc;text-align:center;">
  Auto-generated ¬∑ Gemini {GEMINI_MODEL} ¬∑ arXiv API ¬∑ Last {HOURS_BACK}h
</div>
</body>
</html>"""


# ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def ts() -> str:
    return datetime.now().strftime("%H:%M:%S")


def fetch_and_analyze(papers_raw: list, category: str, label: str) -> tuple:
    """Fetch full text for papers and run Gemini analysis. Returns (result_dict, full_count)."""
    papers = [dict(p, category=category) for p in papers_raw]
    full_count = 0
    print(f"[{ts()}] Fetching full text for {len(papers)} {label} papers...")
    for i, p in enumerate(papers):
        content, is_full = fetch_full_text(p)
        p["content"] = content
        p["is_full"] = is_full
        if is_full:
            full_count += 1
        status = "full" if is_full else "abstract"
        print(f"         [{i+1:2d}/{len(papers)}] {status} ‚Äî {p['title'][:55]}...")
        time.sleep(0.3)
    print(f"[{ts()}] Full text: {full_count}/{len(papers)}")
    if not papers:
        return {"papers": []}, 0
    print(f"[{ts()}] Sending to Gemini ‚Äî {len(papers)} {label} papers...")
    result = call_gemini(build_prompt(papers))
    print(f"         -> {len(result.get('papers', []))} analyzed")
    time.sleep(10)  # polite pause between Gemini calls
    return result, full_count


def analyze_longevity(papers: list) -> dict:
    """Run Gemini analysis on bioRxiv longevity papers using LONGEVITY_PROFILE."""
    if not papers:
        return {"papers": []}
    print(f"[{ts()}] Sending to Gemini ‚Äî {len(papers)} longevity papers...")
    result = call_gemini(build_prompt(papers, profile=LONGEVITY_PROFILE))
    print(f"         -> {len(result.get('papers', []))} analyzed")
    time.sleep(10)
    return result


def main() -> None:
    # 1. Fetch abstracts
    print(f"[{ts()}] Fetching quant papers...")
    quant_new = filter_recent(fetch_arxiv(QUANT_CATEGORIES, max_results=80), HOURS_BACK)[:MAX_QUANT]
    print(f"         -> {len(quant_new)} papers")
    time.sleep(2)

    print(f"[{ts()}] Fetching AI papers...")
    ai_new = filter_recent(fetch_arxiv(AI_CATEGORIES, max_results=60), HOURS_BACK)[:MAX_AI]
    print(f"         -> {len(ai_new)} papers")
    time.sleep(2)

    print(f"[{ts()}] Fetching longevity papers from bioRxiv + medRxiv...")
    longevity_new = fetch_longevity_papers(days_back=4, max_results=MAX_LONGEVITY)
    print(f"         -> {len(longevity_new)} papers")

    if not quant_new and not ai_new and not longevity_new:
        print("No recent papers. Generating empty page.")
        html = generate_html({"papers": []}, {"papers": []}, {"papers": []}, 0, 0, 0, 0, 0)
        os.makedirs("docs", exist_ok=True)
        open(OUTPUT_FILE, "w", encoding="utf-8").write(html)
        return

    # 2. Fetch full text + Gemini analysis per category
    quant_result, quant_full = fetch_and_analyze(quant_new,    "quant", "quant")
    ai_result,    ai_full    = fetch_and_analyze(ai_new,       "ai",    "AI")
    longevity_result         = analyze_longevity(longevity_new)

    total_full  = quant_full + ai_full
    total_count = len(quant_new) + len(ai_new)

    # 3. Generate HTML
    html = generate_html(
        quant_result, ai_result, longevity_result,
        len(quant_new), len(ai_new), len(longevity_new),
        total_full, total_count,
    )
    os.makedirs("docs", exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as fh:
        fh.write(html)
    print(f"[{ts()}] Done -> {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
